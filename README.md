# Multi-Layer Perceptron with Stochastic Gradient Descent

This repository contains an implementation of a Multi-Layer Perceptron (MLP) neural network trained using stochastic gradient descent (SGD). The network is designed to classify handwritten digits from the MNIST dataset, achieving an impressive classification score of 93%.

## Overview

The Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network composed of multiple layers of nodes, each connected to the next in a feedforward manner. It is a versatile architecture capable of learning complex relationships in data. Stochastic Gradient Descent (SGD) is a popular optimization algorithm used to train neural networks by updating the weights iteratively based on small random batches of data.

## Features

- **Multi-Layer Perceptron (MLP)**: A customizable neural network architecture consisting of an input layer, one or more hidden layers, and an output layer.
- **Stochastic Gradient Descent (SGD)**: An optimization algorithm used to train the MLP by updating the weights based on randomly sampled mini-batches of data.
- **MNIST Dataset**: A benchmark dataset containing handwritten digits, commonly used for training and testing machine learning models.
- **93% Classification Score**: The trained MLP achieves a classification accuracy of 93% on the MNIST test dataset, demonstrating its effectiveness in digit recognition tasks.

## Requirements

- Python 3.x
- NumPy
- tensorflow (for MNIST data set)
